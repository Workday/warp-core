[
{
	"uri": "https://workday.github.io/warp-core/contents/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ve all been there before: struggling to reason about why a function is not performing the way we expect, and whether a candidate replacement that may appear faster actually bring a statistically significant improvement to the table.\nWe on the performance-test-frameworks team have noticed that engineers often reason about the performance of their code in a very informal way.\nAcademic papers published in other scientific fields, such as biology, place an emphasis on obtaining statistically significant results and sound experimental design. Unfortunately, the field of computer science tends to not place such importance on statistical rigor. This can make it difficult to draw meaningful conclusions, or even worse, can lead us to arrive at misleading or downright incorrect conclusions.\nTo help engineers reason about performance in a more scientific way, We\u0026rsquo;re proud to introduce WARP (Workday Automated Regression Platform): a flexible, lightweight, (mostly) functional Scala framework for measuring and comparing the performance of arbitrary code blocks.\nThe goal of this library is to make it easier to apply the scientific method to the performance testing process. Our intended user audience consists of perforrmance optimization engineers, test engineers, and ML researchers. We at Workday believe engineers should reason scientifically about code performance.\nThis library is inspired in part by this research on statistically rigorous performance evaluation techniques for Java.\nWARP features a DSL for describing experimental designs (conducting repeated trials for different experimental groups). Additionally, we support collecting custom measurements, expressing custom performance requirements, and conducting statistical significance tests on history of two tests.\nCollected measurements are written to a relational database for subsequent view and analysis. WARP has been tested on both H2 and MySQL.\nWARP framework includes features that make it simple to:\n collect arbitrary custom measurements for a test invocation. repeatedly invoke a test, collecting measurements for each invocation. exclude initial invocations from being measured (for example, to allow the JVM to warmup, or perform JIT). distribute all test invocations to a threadpool of configurable size. inject statistical delay between invocations (load testing). group tests for the purpose of determining significant statistical differences. tag test invocations with custom identifiers (JIRA ticket, A/B testing). define custom failure conditions (Arbiters)  The general WARP framework has two supported frontends:\n JUnit annotations and extensions (for Java or Scala users)  import com.workday.warp.junit.WarpTest; import org.junit.jupiter.api.Assertions; public class ExampleTest { @WarpTest(warmups = 2, trials = 6) public void example() { Logger.info(\u0026#34;hello, world!\u0026#34;); Assertions.assertEquals(2, 1 + 1); } }  DSL (for Scala users only)  import org.junit.jupiter.api.{Test, TestInfo} import com.workday.warp.dsl._ import com.workday.warp.junit.WarpJUnitSpec class ExampleSpec extends WarpJUnitSpec { @Test def example(testInfo: TestInfo): Unit = { using testId testInfo invocations 8 threads 4 measuring { someExperiment() } should not exceed (1 second) } } The above DSL example creates a pool of 4 threads, invokes the given test 8 times, collects a default set of measurements from each invocation, and writes the results to a relational database.\nWARP includes many features that make it easy to reason scientifically about your performance tests, such as statistical significance tests and anomaly detection to help find performance regressions.\nInternally, we use WARP in several different ways:\n detecting performance regressions in test suites in our CI environment ad-hoc performance testing in response to reported performance issues statistically evaluating multiple method implementations (A/B testing)  We are excited to provide a platform to help engineers reason about their code performance in a more principled manner.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/getting_started_java/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Warp-core allows users to instrument and persist measurements collected for their tests. The primary key warp-core uses to identify individual tests is a fully qualified test method signature. We refer to this as a \u0026ldquo;TestId\u0026rdquo;. Warp-core is implemented using the JUnit5 extension model. Existing JUnit tests can be annotated to repeatedly execute or record telemetry. The most basic example is a plain JUnit test annotated with \u0026ldquo;@WarpTest\u0026rdquo;:\nJava: import com.workday.warp.junit.WarpTest; import org.junit.jupiter.api.Assertions; public class ExampleTest { /** A test that will be invoked a total of 6 times, 2 unmeasured warmups and 4 measured trials. */ @WarpTest(warmups = 1, trials = 2) public void measured() { Assertions.assertEquals(2, 1 + 1); } }\nScala: import com.workday.warp.junit.WarpTest import org.junit.jupiter.api.Assertions class ExampleSpec { /** A test that will be invoked a total of 6 times, 2 unmeasured warmups and 4 measured trials. */ @WarpTest(warmups = 1, trials = 2) def measured(): Unit = { Assertions.assertEquals(2, 1 + 1) } }\n\u0026rdquo;@WarpTest\u0026rdquo; is a meta-annotation that combines JUnit5 \u0026ldquo;@TestTemplate\u0026rdquo; annotation with our \u0026ldquo;WarpTestExtension\u0026rdquo; JUnit extension. A JUnit test template is not directly a test case, but is rather a template designed to be invoked multiple times as dictated by invocation context providers. \u0026ldquo;WarpTestExtension\u0026rdquo; is an invocation context provider that also uses JUnit \u0026ldquo;@BeforeEach\u0026rdquo; and after hooks to insert calls into our persistence module via the \u0026ldquo;MeasurementExtension\u0026rdquo;\nIf your project has other constraints that preclude you from using \u0026ldquo;@TestTemplate\u0026rdquo; instead of \u0026ldquo;@Test\u0026rdquo;, another possibility is adding the \u0026ldquo;@Measure\u0026rdquo; annotation to your existing tests, however note that this approach does not support repeated measurements or warmups.\nJava: import com.workday.warp.junit.Measure; import org.junit.jupiter.api.Assertions; import org.junit.jupiter.api.Test; public class ExampleTest { @Test @Measure public void measureExtension() { Assertions.assertEquals(2, 1 + 1); } }\nScala: import com.workday.warp.junit.Measure import org.junit.jupiter.api.{Assertions, Test} class ExampleSpec { @Test @Measure def measureExtension(): Unit = { Assertions.assertEquals(2, 1 + 1) } }\nOccasionally users may require usage of a lower-level api and direct access to a \u0026ldquo;TestId\u0026rdquo;. At the database level, a \u0026ldquo;TestId\u0026rdquo; is used as a unique test identifier, stored as a fully qualified test method name. For this use case we provide implicits augmenting Junit \u0026ldquo;TestInfo\u0026rdquo;. A \u0026ldquo;TestInfo\u0026rdquo; is available to all JUnit tests using a default \u0026ldquo;ParameterResolver\u0026rdquo; that is automatically configured for all tests. Java users can call \u0026ldquo;TestId.fromTestInfo\u0026rdquo; directly, while scala users can make use of an implicit conversion:\nJava: import com.workday.warp.TestId; import com.workday.warp.junit.WarpTest; import org.junit.jupiter.api.Assertions; import org.junit.jupiter.api.TestInfo; public class ExampleTest { @WarpTest public void testId(final TestInfo info) { final String id = TestId.fromTestInfo(info).id(); Assertions.assertTrue(\u0026#34;com.workday.warp.examples.ExampleTest.testId\u0026#34;.equals(id)); } }\nScala: import com.workday.warp.TestIdImplicits._ import com.workday.warp.junit.WarpTest import org.junit.jupiter.api.{Assertions, TestInfo} class ExampleSpec { @WarpTest def testId(info: TestInfo): Unit = { // TestIdImplicits implicit conversion  val testId: String = info.id Assertions.assertEquals(\u0026#34;com.workday.warp.examples.ExampleTest.testId\u0026#34;, testId) } }\nAlternatively, we also provide a \u0026ldquo;ParameterResolver\u0026rdquo; that allows resolution of \u0026ldquo;WarpInfo\u0026rdquo;. \u0026ldquo;WarpInfo\u0026rdquo; is similar to Junit \u0026ldquo;TestInfo\u0026rdquo;, but also allows users to access metadata about current test iteration sequences. Note, however, that this parameter resolver is tightly coupled to warp-core invocation context extensions, and will only work for tests annotated with \u0026ldquo;@WarpTest\u0026rdquo;.\nJava: package com.workday.warp.examples; import com.workday.warp.junit.WarpTest; import org.junit.jupiter.api.Assertions; public class ExampleTest { /** Annotated WarpTests can also use the same parameter provider mechanism to pass WarpInfo. */ @WarpTest public void measuredWithInfo(final WarpInfo info) { Assertions.assertEquals(\u0026#34;com.workday.warp.examples.ExampleTest.measuredWithInfo\u0026#34;, info.testId()); } }\nScala: package com.workday.warp.examples import com.workday.warp.junit.WarpTest import org.junit.jupiter.api.Assertions class ExampleSpec { /** Annotated WarpTests can also use the same parameter provider mechanism to pass WarpInfo. */ @WarpTest def measuredWithInfo(info: WarpInfo): Unit = { Assertions.assertTrue(\u0026#34;com.workday.warp.examples.ExampleTest.measuredWithInfo\u0026#34;, info.testId) } }\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/getting_started_scala/",
	"title": "Scala DSL",
	"tags": [],
	"description": "",
	"content": " The recommended way to interact with WARP is through the Scala DSL. This API provides a richer feature set than the Java API, including the ability to register custom MeasurementCollector and Arbiter instances, and add new tags in the form of String metadata that will be persisted.\nThe DSL is implemented by using an immutable case class, ExecutionConfig to hold all configuration parameters, such as number of invocations, warmups, threadpool size, etc. We give sane defaults to all parameters. Calls to the various DSL methods invoke the compiler-generated copy method to build up new instances of ExecutionConfig: import com.workday.warp.dsl._ val config: ExecutionConfig = using invocations 32 threads 4 distribution GaussianDistribution(50, 10)\nNote, however, that the DSL itself manages the measurement lifecycle. Thus, we do not recommend using \u0026ldquo;@WarpTest\u0026rdquo; annotation together with the DSL, as that would lead to doubly measured tests. The DSL can be especially useful in cases where users already make heavy use of \u0026ldquo;BeforeEach\u0026rdquo;/\u0026ldquo;AfterEach\u0026rdquo; hooks. \u0026ldquo;@WarpTest\u0026rdquo; annotation is implemented using JUnit before/after hooks, the order of which cannot be controlled. Thus, it is possible that tests using \u0026ldquo;@WarpTest\u0026rdquo; will have extra overhead from other hooks included in their measurement metadata. The DSL is decoupled from JUnit and can be used with other JVM testing frameworks.\nFinally, a call-by name block is passed to ExecutionConfig.measuring:\nimport com.workday.warp.dsl._ import com.workday.warp.TestIdImplicits._ import org.junit.jupiter.api.{Test, TestInfo} class ExampleSpec extends WarpJUnitSpec { @Test def dslExample(testInfo: TestInfo): Unit = { using testId testInfo invocations 32 threads 4 measuring { val i: Int = 1 Logger.info(s\u0026#34;result is ${i + i}\u0026#34;) } should not exceed (2 seconds) } } Custom Arbiter and MeasurementCollector instances can be registered by calling the arbiters and collectors methods: import com.workday.warp.dsl._ import com.workday.warp.TestIdImplicits._ import org.junit.jupiter.api.{Test, TestInfo} class ExampleSpec extends WarpJUnitSpec { @Test def dslCollectors(testInfo: TestInfo): Unit = { // disables the existing default collectors, and registers a new collector  using testId testInfo only these collectors { new SomeMeasurementCollector // registers two new arbiters  } arbiters { List(new SomeArbiter, new SomeOtherArbiter) } measuring { someExperiment() } should not exceed (5 seconds) } }\nThe arbiters and collectors methods both accept a call-by-name function that returns an Iterable. We use an implicit to lift a single instance into an Iterable type.\nThe threshold defined by the should not exceed syntax is implemented as a scalatest Matcher[Duration].\nDSL Operations The DSL provides a flexible way to describe experimental setups for conducting repeated trials, and supports the following operations:\n testId(id: TestId) sets a testId, the name under which results will be recorded in our database. (default \u0026quot;com.workday.warp.Undefined.undefined\u0026quot;. Typically we use the fully qualified method name of the test being measured.)\n invocations(i: Int) sets the number of measured trial invocations (default 1).\n warmups(w: Int) sets the number of unmeasured warmups (default 0).\n threads(p: Int) sets the thread pool size (default 1).\n distribution(d: DistributionLike) sets a Distribution to govern expected delay between scheduling invocations (default 0 delay).\n mode(m: ModeWord) sets a \u0026ldquo;mode\u0026rdquo; for test measurement. This is an advanced feature that only applies to experiments with a threadpool of at least 2 threads. The single mode will treat the entire schedule of invocations as a single logical test. A single controller will be created to measure the entire schedule. The multi mode (which is the default) measures each invocation on an individual basis.\n only defaults is a no-op included for more English-like readability.\n no arbiters disables all existing default Arbiter.\n arbiters(a: =\u0026gt; Iterable[ArbiterLike]) registers a collection of new Arbiter to act on the results of the measured test.\n only these arbiters(a: =\u0026gt; Iterable[ArbiterLike]) composes the two above operations; disabling all existing arbiters and registering some new arbiters.\n no collectors disables all existing default measurement collectors.\n collectors(c: =\u0026gt; Iterable[AbstractMeasurementCollector]) registers a collection of new measurement collectors to measure the given test.\n only these collectors(c: =\u0026gt; Iterable[AbstractMeasurementCollector]) composes the two above operations; disabling all existing collectors before registering some new collectors.\n tags(t: =\u0026gt; Iterable[Tag]) registers a sequence of Tags that will be applied to the trials.\n measure(f: =\u0026gt; T) (and its synonym, measuring) perform the measurement process wrapped around f. We include the measuring synonym solely for more English-like readability when combined with a static threshold using our scalatest Matcher[Duration]: using only defaults measure someExperiment using only defaults measuring someExperiment should not exceed (5 seconds) This is naturally the most complex operation, as it involves creating a threadpool, layering collectors in the correct order, persisting results, etc.\n  DSL Return Type After we have constructed an ExecutionConfig that fits the needs of our experiment, we call the measuring (or measure, a synonym) method with the call-by-name function f that we want to measure. The return type of measuring is List[TrialResult[T]], where T is the return type of f. This gives us access to the returned values of each invocation of f if they are needed for further analysis.\nAdditionally, TrialResult holds a TestExecutionRow corresponding to the newly written row in our database, and the measured response time of the trial.\nFor example, suppose we are interested in measuring the performance of List.fill[Int] construction:\nimport com.workday.warp.dsl._ import com.workday.warp.utils.Implicits._ import com.workday.warp.TestIdImplicits._ import org.junit.jupiter.api.{Test, TestInfo} class ExampleSpec extends WarpJUnitSpec { @Test def listFill(testInfo: TestInfo): Unit = { // measure creating a list of 1000 0\u0026#39;s  val results: Seq[TrialResult[List[Int]]] = using testId testInfo invocations 8 measure { List.fill(1000)(0) } // make some assertions about the created lists  results should have length 8 results.head.maybeResult should not be empty for { result \u0026lt;- results list \u0026lt;- result.maybeResult element \u0026lt;- list } element should be (0) } } After measurement has been completed, we are able to access the return values of the function being measured.\nThe DSL provides a flexible way to customize the execution schedule of your experiment, including adding new measurement collectors and arbiters for defining failure criteria.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/custom_collectors/",
	"title": "Collecting Custom Measurements",
	"tags": [],
	"description": "",
	"content": "The AbstractMeasurementCollector class defines two side-effecting functions: startMeasurement and stopMeasurement, that are invoked, respectively, before and after a test execution.\nImplementations of a measurement or measurement collection should extend this class, and probably mix in the CorePersistenceAware trait. Note that the collection of a measurement may encompass a number of different types of operations. It may include such activities as collecting from JMX, scraping server logs, or as simple as starting and stopping a clock.\nOut of the box, WARP includes a default minimal set of measurement collectors for measuring elapsed wall-clock time and heap usage. Internally, our engineers have developed many additional collectors for instrumenting their code.\nFor example, the following class records elapsed wall clock time as a measurement:\nimport com.workday.warp.collectors.AbstractMeasurementCollector import com.workday.warp.persistence.CorePersistenceAware import com.workday.warp.persistence.TablesLike._ import com.workday.warp.persistence.Tables._ class WallClockTimeCollector extends AbstractMeasurementCollector with CorePersistenceAware { private var timeBeforeMs: Long = _ /** Called prior to a test invocation. */ override def startMeasurement(): Unit = { this.timeBeforeMs = System.currentTimeMillis() } /** Called after finishing a test invocation. */ override def stopMeasurement[T: TestExecutionRowLikeType](maybeTestExecution: Option[T]): Unit = { val durationMs: Long = System.currentTimeMillis() - this.timeBeforeMs maybeTestExecution foreach { testExecution: T =\u0026gt; this.persistenceUtils.recordMeasurement(testExecution.idTestExecution, \u0026#34;wall clock time\u0026#34;, durationMs) } } } In some cases it may be desirable to instead record measurements at a periodic interval throughout the duration of a test. The trait ContinuousMeasurement implements this functionality by spawning a separate thread that can run arbitrary code. To create a continuous measurement collector, subclass AbstractMeasurementCollector, mix in the ContinuousMeasurement trait, and override the method collectMeasurement().\nAbstractMeasurementCollector provides a flexible interface for extending the core framework with additional measurements.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/custom_arbiters/",
	"title": "Custom Arbiters",
	"tags": [],
	"description": "",
	"content": " An Arbiter defines a vote method that allows us to implement custom failure criteria for a given test execution.\nFor example, the following Arbiter implementation fails a test if it took longer than 5 seconds: import com.workday.warp.arbiters.ArbiterLike import com.workday.warp.persistence.CorePersistenceAware import com.workday.warp.persistence.TablesLike._ import com.workday.warp.persistence.Tables._ class ResponseTimeArbiter extends ArbiterLike with CorePersistenceAware { /** * Checks that the measured test took less than 5 seconds. * * @param ballot box used to register vote result. * @param testExecution [[TestExecutionRowLikeType]] we are voting on. * @return a wrapped error with a useful message, or None if the measured test passed its requirement. */ override def vote[T: TestExecutionRowLikeType](ballot: Ballot, testExecution: T): Option[Throwable] = { val testId: String = this.persistenceUtils.getMethodSignature(testExecution) if (testExecution.responseTime \u0026gt; 5) { Option(new RequirementViolationException(s\u0026#34;$testIdtook longer than 5 seconds\u0026#34;)) } else { None } } }\nSome arbiters may want access to historical data in order to make an informed decision about the most recent test invocation. To easily access historical data, mix in the trait CanReadHistory. For example, using this mechanism, we can implement an arbiter that votes based on the z-score of the most recently test execution. Using such an arbiter, for example, we can fail any test whose recorded measurement value is greater standard deviations away from the historical mean.\nAnomaly Detection WARP includes an arbiter that uses machine learning to cast votes. The RobustPcaArbiter votes based on the results of an anomaly detection algorithm, Robust Principal Component Analysis (RPCA). Internally, we use this arbiter to fail any tests that have suspicious or anomalous measurements. See the dedicated RPCA page here for a more detailed description of how this algorithm works, including our novel algorithmic extension designed to improve strictness and avoid false negatives over time.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/measurement_collection_controller/",
	"title": "Measurement Collection Controller",
	"tags": [],
	"description": "",
	"content": "The MeasurementCollectionController (often called MCC) is the central \u0026ldquo;overlord\u0026rdquo; class responsible for tracking all registered collectors and arbiters, wrapping collectors around measured tests in the correct order, and invoking the measured code block.\nNote that all collectors must be stopped in the reverse order from which they were started to maximize measurement accuracy and avoid mistakenly capturing any overheard from other collectors.\nThe multiple circles of hell serve as a useful mental model: each circle represents a collector that is started/stopped in a specific order.   MCC is the lowest-level api we provide, and it is possible to use this class directly:\nimport com.workday.warp.controllers.DefaultMeasurementCollectionController import org.junit.jupiter.api.{Test, TestInfo} class ExampleSpec { // this example manages the measurement lifecycle directly and thus uses @Test instead of @WarpTest  @Test def mcc(testInfo: TestInfo): Unit = { val mcc = new DefaultMeasurementCollectionController(testId = testInfo) mcc.registerCollector(new SomeMeasurementCollector) // start all measurement collectors prior to running the experiment  mcc.beginMeasurementCollection() // run your experiment code  someExperiment() // stop all measurement collectors, persist results, other cleanup  mcc.endMeasurementCollection() // alternate signature that allows for manually specifying elapsed test time and a test timing threshold  // mcc.endMeasurementCollection(elapsedTime = 5.seconds, threshold = 6.seconds)  } } The DSL encapsulates the above sequence of operations in a more convenient API and provides some higher-level configuration options, such as distributing multiple test invocations onto a threadpool.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": "Tags are simply key-value pairs that are attached to entities in the database.\nWe support adding tags to TestDefinition, TestExecution, and other tags themselves in the form of metatags.\nInternally, we use tags to differentiate multiple dataset series, annotate tests under triage with their ticket numbers, and record additional metadata about the environment in which a test was executed.\nTags can be configured at the start of test using the DSL.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/runtime_configuration/",
	"title": "Runtime Property Configuration",
	"tags": [],
	"description": "",
	"content": "WARP has many runtime configuration options. For example, one can set the log level, or override default JDBC persistence properties.\nConfiguration properties can be set using JVM system properties or by creating a configuration file.\nFor example, the default database we write to is an in-memory H2 database, however the following minimal warp.properties file can be used as a starting point to write results to MySQL:\nwd.warp.jdbc.url=jdbc:mysql://localhost:3307/warp?createDatabaseIfNotExist=true\u0026amp;zeroDateTimeBehavior=convertToNull wd.warp.jdbc.password=1234 wd.warp.jdbc.user=root wd.warp.jdbc.driver=com.mysql.jdbc.Driver wd.warp.log.level=info  By default, WARP will search for a warp.properties file in the current working directory and in ~/.warp/, however the expected location of the properties file can be overridden using the JVM system property wd.warp.config.directory.\nThere are many tunable knobs; for more detail, see WarpPropertyManager, CoreWarpProperty, and PropertyEntry\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/persistence/",
	"title": "Persistence",
	"tags": [],
	"description": "",
	"content": " WARP uses slick for our persistence library, and flyway for managing database migrations.\nSlick can have a steep learning curve for new developers; we recommend reading Essential Slick as a primer for those looking to get more acquainted with the library.\nOur persistence module is designed to be extensible for custom needs. Internally, we augment the default schema with some additional proprietary measurement columns. See the advanced schema page here for a more detailed description of our persistence architecture and how you can extend our schema to fit your needs.\nBasic Concepts The primary tables we define are Build, TestDefinition and TestExecution. Build allows users to track information about the version of the service they are testing. TestDefinition represents the logical definition of a test that may potentially be executed many times. Typically, we use the fully qualified method signature of the test as its identity key. TestExecution represents a single execution of a test, and has pointers to the corresponding TestDefinition and Build the test was executed under.\nSlick We make heavy use of the slick code generator for generating boilerplate implicits and typeclasses.\nFor example, the following generated case class is used to represent a single test execution:\ncase class TestExecutionRow(override val idTestExecution: Int, override val idTestDefinition: Int, override val idBuild: Int, override val passed: Boolean, override val responseTime: Double, override val responseTimeRequirement: Double, override val startTime: java.sql.Timestamp, override val endTime: java.sql.Timestamp) extends TestExecutionRowWrapper(idTestExecution, idTestDefinition, idBuild, passed, responseTime, responseTimeRequirement, startTime, endTime) Queries The queries we use are defined in CoreQueries. To obtain access to the configured database, simply mix in the trait CorePersistenceAware to obtain an instance of CorePersistenceUtils\nCorePersistenceUtils contains utility methods for interacting with the database.\nFor example, the createTestExecution method is used to write a record of a single test execution:\n/** * Creates, inserts, and returns a [[TestExecutionRowLike]] * * @param testId id of the measured test (usually fully qualified junit method). * @param timeStarted time the measured test was started. * @param responseTime observed duration of the measured test (seconds). * @param maxResponseTime maximum allowable response time set on the measured test (seconds). * @return a [[TestExecutionRowLike]] with the given parameters. */ override def createTestExecution(testId: TestId, timeStarted: Instant, responseTime: Double, maxResponseTime: Double): TestExecutionRowLike\nSimilarly, the recordMeasurement method is used to persist a measurement obtained for a given test execution.\n/** * Persists generic measurements in the Measurement table. Looks up the MeasurementName corresponding to * `name`, creates a new Measurement with the appropriate fields set. * * @param idTestExecution id for the [[TestExecutionRow]] associated with this measurement. * @param name name to use for this measurement. * @param result result of this measurement. */ override def recordMeasurement(idTestExecution: Int, name: String, result: Double): Unit = { In general, if you need to interact with the WARP database, mix CorePersistenceAware into your class and use the provided instance of CorePersistenceUtils.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/dependency_injection/",
	"title": "Dependency Injection",
	"tags": [],
	"description": "",
	"content": "WARP uses Guice for some dependency bindings.\nIn particular, we bind a concrete implementation of AbstractMeasurementCollectionController, an implementation of WarpPropertyLike (which enumerates all the available runtime configuration parameters), and provide an extension hook for users to bind additional tinylog Writers.\nThe binding module we provide out of the box in DefaultWarpModule should be fine for most use cases. However, advanced users can implement their own bindings in a new module and set the system property wd.warp.inject.module with the fully qualified value of their module class.\nInternally, we use an augmented MeasurementCollectionController, a slightly different schema with different implementations for PersistenceUtils, and an extended version of CoreWarpPropery.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/gatling/",
	"title": "Measuring Gatling Simulations",
	"tags": [],
	"description": "",
	"content": "\u0026ldquo;warp-core-gatling\u0026rdquo; is published as a separate jar and includes a base class \u0026ldquo;WarpSimulation\u0026rdquo; that add warp measurements to existing simulations as before/after gatling hooks. This module also includes a (now deprecated) vintage JUnit4 runner that allows for easily executing gatling simulations along with the rest of your JUnit test suite. In the future, we plan to develop a tighter integration between gatling and JUnit5\nimport com.workday.warp.adapters.gatling.{GatlingJUnitRunner, WarpSimulation} @RunWith(classOf[GatlingJUnitRunner]) class BasicSimulation extends WarpSimulation { val httpConf: HttpProtocolBuilder = http .baseUrl(\u0026#34;http://google.com\u0026#34;) val scn: ScenarioBuilder = scenario(\u0026#34;Positive Scenario\u0026#34;) .exec( http(\u0026#34;request_1\u0026#34;).get(\u0026#34;/\u0026#34;) ) setUp(scn.inject(atOnceUsers(1)).protocols(httpConf)) } "
},
{
	"uri": "https://workday.github.io/warp-core/contents/anomaly_detection/",
	"title": "Case Study: RPCA Anomaly Detection",
	"tags": [],
	"description": "",
	"content": " One of the arbiters included with WARP is the RobustPcaArbiter.\nThis arbiter is based on an anomaly detection algorithm called Robust Principal Component Analysis. (RPCA). RPCA is a matrix decomposition algorithm that decomposes a measurement matrix M into the sum L + S + E. L is a low-rank matrix that represents the \u0026ldquo;normal\u0026rdquo; component of M. S is a sparse matrix that represents the anomalies in M, and E is an error term representing random noise.\nRPCA has other applications besides anomaly detection. For example, it can be used to segment foreground/background in a surveillance video stream. L naturally corresponds to the static background, while S captures objects moving in the foreground. Similarly, RPCA can be used to remove specularities, shadows, or other irregular artifacts from photos as a preprocessing step to facial recognition; and to find irregular words as part of a spam detection or web indexing pipeline.\nMore detailed information can be found in the Candes09 and Zhou10 papers.\nWe use this arbiter internally to detect anomalies in recorded test measurements.\nFor example, a test suddenly running much slower may signify a performance regression that needs to be further investigated.\nBackground In our internal legacy pipeline, test owners would set static thresholds on their tests for the purpose of detecting regressions due to degraded performance. This led to several problems:\n predicting the expected performance on production hardware is difficult a priori. updating thresholds over time puts onerous requirements on test owners.  We wanted to retain these threshold semantics because they are easy to reason about, but we wanted to remove the requirement of manual input from developers.\nWhat if we could learn a suitable threshold, given the historic measurements for a test?\nThe SmartNumberArbiter combines the bisection method with RPCA to efficiently find the minimum measurement value that would be flagged as anomalous. This derived threshold is recorded as additional test metadata (using a tag), making the arbitration process easier to reason about than using a raw RobustPcaArbiter.\nInternally, we use RPCA daily to detect anomalies in an online manner for test executions. This chart is taken from an internal dashboard and illustrates how the detected anomalies can look over time:   This code snippet illustrates how the RobustPcaArbiter can be used with an experiment:\n@Test def rpcaExample(): Unit = { using invocations 20 arbiters { new RobustPcaArbiter } measure { someExperiment() } } Tuning RPCA has several parameters that can be tuned for modifying the sensitivity of the algorithm. We use the defaults described in the Zhou10 paper as much as possible, however one parameter we encourage developers to experiment with is called tolerance factor. This is essentially a threshold in the normalized space of S, and directly controls how sensitive the algorithm is. Smaller values are more sensitive, while larger values give the algorithm more slack in determining whether a new test is anomalous. The tolerance factor is controlled via the property wd.warp.anomaly.rpca.s.threshold.\nDouble RPCA However, one weakness of Robust PCA is that the system has a \u0026ldquo;short memory\u0026rdquo;. In other words, a sustained performance degradation will quickly be considered normal.\nTo remedy this, we developed a novel extension to RPCA called Double RPCA. This algorithm features an \u0026ldquo;extended training phase\u0026rdquo; where only past normal measurements are used to determine the current threshold value. One intuitive way to grasp the effects of this modification is to say that it, roughly speaking, \u0026ldquo;prevents Stockholm Syndrome\u0026rdquo;.\nThis variant is more strict with respect to sustained test performance degradation, and can lead to increased false positive rates over extended periods of time. This algorithm is a good fit for strictly judging the performance of some core functionality, especially code with constant-time asymptotic complexity. If the performance of a certain critical code block is never expected to change, Double RPCA is a good fit.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/hypothesis_testing/",
	"title": "Case Study: Hypothesis Testing",
	"tags": [],
	"description": "",
	"content": " Two of the most challenging aspects of experimental design are the creation of a strong testable hypothesis and devising an experiment to test it. This article will describe features in WARP designed to assist users in evaluating their experimental hypotheses.\nThanks to Michael Ottati for writing the original form of this document.\nThe Hypothesis A hypothesis is a theory or speculation typically originating from a question:\n If X changes, what happens to Y?\n A question can be converted into a hypothesis by reformulating it as a statement.\n A change in X causes a change in Y.\n A directional hypothesis is stronger form of hypothetical that predicts the relationship of the change in the variables, e.g. when X increases Y increases along with it, a direct relationship. An inverse relationship, the prediction that an increase in X causes a corresponding decrease in Y is another form of directional hypothesis.\nThe remainder of this article will illustrate how an engineer might use WARP to validate the following (obvious) directional hypothesis:\n If we replace binary search on a sufficiently large Vector[Int] with linear search, performance will degrade.\n The Null Hypothesis A null hypothesis is a hypothesis that states there is no statistical significance between the two variables of the hypothesis. Hypothesis testing begins with disproving (falsifying) the null hypothesis. Once the null hypothesis has been disproven, the experimenter has grounds for concluding that a relationship exists between the variables. In our case, the null hypothesis states that there is no significant difference between linear search and binary search. Of course, we can never accept the null hypothesis, only fail to reject it.\nThe Test Environment In order to validate the assertion that: A change in X causes a change in Y one must first validate that: Constant X yields constant Y. Stated differently, the basis for any performance test must be a provably repeatable test.\nIn cases where constant X does not yield constant Y, it may be due to noise in your test environment. There are many causes of noise in computer systems: unaccounted for background processing, periodic task execution, and many others.\nPerformance tests should be based upon augmentations of demonstrably repeatable tests, and run in noise free environments. We will construct our experiment by constructing it on top of one such test.\nA repeatable test is a mandatory precondition upon which to test, and ideally validate, our hypothesis. No valid conclusion can be drawn from a test that is itself not repeatable.\nIdempotency is an important concept in computer science, much literature has been written on this topic. It is often difficult to design idempotent tests for systems that mutate state. It is sometimes impossible to design an idempotent test when the thing being tested mutates (or destroys) the underlying state necessary to rerun it. \u0026ldquo;Fire all Employees\u0026rdquo; is a hypothetical example of such a test, it can only be meaningfully run once. This What is idempotency? article provides a short mildly humorous tutorial on the topic.\nExogenous factors affecting repeatability Warmups When running performance tests, it is almost always the case that the first invocation of a test behaves differently than all subsequent runs of the same test. This is a well known issue that has several common and well understood causes. It is nearly always the case that the first execution through any code path involves considerably more processing than subsequent invocations. The list below enumerates some common reasons for these differences, it is by no means exhaustive.\n Operating System Cache JVM Class Loading JIT Compilation Execution Code Path Differences the first time through. (lazy loading, application cache, \u0026hellip;)  The performance effects of first time execution are well known, well understood, and ubiquitous. Nearly every performance tool of any sophistication includes a \u0026ldquo;warm up\u0026rdquo; capability for this reason. WARP includes a simple mechanism within its test execution launcher to specify warmups and invocations. By manipulating these two properties you may specify how many unrecorded warmups will be run, as well as how many invocations should be recorded into your final results.\nOperating System Drift Performance tests are exquisitely sensitive to the environments they run in. A performance test run on your laptop will not yield the same performance on a standalone host in a data center. It is important to remain mindful of this when running tests on different platforms. The fact that the same test run on different hardware performs differently is intuitively obvious to most people.\nWhat is less obvious, is the fact that the same test run on the same machine may also perform differently over time. Internally, we saw an example of this recently after Meltdown and Spectre patches were applied to several systems in our lab. These patches caused an across the board performance degradation for all tests running in our labs.\nMachines can also degrade over time in unexpected ways. It is therefore important to periodically calibrate machines to ensure that their performance is comparable to other hosts that are thought to be identical. After the Meltdown patches were applied, some of the machines in our lab drifted apart from each other. They were brought back into sync with each other, only by re-imaging all of them.\nChange one parameter, measure, repeat. If more than a single test parameter is changed between two test runs, no valid conclusion can be made about the result. In order to fully disambiguate the causality of a change, the experimenter must vary only a single parameter at a time. When more than one parameter is varied between two test runs, it is impossible to attribute back to either individual paramater what portion of the resulting change each individual parameter was responsible for.\nImagine the experiment of switching gas brands to determine if the new brand provides better gas mileage. Your test design might be to drive along previously traveled route where you have recorded consistent gas mileage in the past. Prior to starting your test, your neighbor asks you to tow his boat to your destination. You agree since you are traveling there anyway.\nThis experiment run without the boat may or may not have yielded better gas mileage. The second variable of extra weight so dominated the results however that it led to the (possibly) incorrect conclusion that gas mileage got worse. When multi variable changes are made it is impossible to amortize the effects of the individual changes or to even know if one of the changes leads to a worse result than would have been the case if it had not been changed at all.\nChanging a single thing at a time is the golden rule of hypothesis testing, it is also the rule most frequently violated, often leading to fallacious results.\nTest Design In order to test our hypothesis, we must vary a single parameter: using binary search vs linear search. We also know that it is good practice to run a series of invocations in each configuration to verify that results are repeatable in both configurations. Finally, it is a good practice to warm up the system by running a few test runs prior to recording the data we are going to analyze.\nIn addition to running our test multiple times in each configuration, we need some way to record how the test was run so we can later understand our result data. WARP has several features that will assist us in this task. The most important of feature is tagging. Tags are attributes that can be configured, and recorded with each test run.\nTags are simply key-value pairs that are recorded in the results database, and joined with the test execution. The easiest way to think about tags is that they are an extensible attribute set that allow the recording of variable state at the time your test is run. For this experiment, we can name our tag search-type and we can record two values for it to discriminate our series: binary|linear\nCode Setup Suppose we have the two following search implementations:\n Binary Search  def binarySearch(numbers: Vector[Int], target: Int): Boolean = { @tailrec def helper(left: Int, right: Int): Boolean = { val mid: Int = left + ((right - left) / 2) val midElement: Int = numbers(mid) if (right - left \u0026lt; 1) false else if (midElement \u0026lt; target) helper(left, mid - 1) else if (midElement == target) true else helper(mid + 1, right) } helper(0, numbers.length - 1) }  Linear Search  @tailrec private def linearSearch(numbers: Vector[Int], target: Int, i: Int = 0): Boolean = { if (i == numbers.length) false else if (numbers(i) == target) true else linearSearch(numbers, target, i + 1) } As good computer scientists, we know that binary search will significantly outperform the linear variant on sufficiently large datasets. Here is how we could elucidate that fact using WARP to capture test statistics:\n@Test def hypothesisExample(): Unit = { // randomly generate a dataset, and a collection of numbers to search for.  val dataset: Vector[Int] = Vector.fill(10000000)(Random.nextInt(100)).sorted val targets: Seq[Int] = List.fill(10)(Random.nextInt(100)) // run both search algorithms with the DSL.  val linearResults: Seq[TrialResult[_]] = using only defaults warmups 5 invocations 30 tags { List(ExecutionTag(\u0026#34;search-type\u0026#34;, \u0026#34;linear\u0026#34;)) } measure { targets.foreach(i =\u0026gt; linearSearch(dataset, i)) } val binaryResults: Seq[TrialResult[_]] = using only defaults warmups 5 invocations 30 tags { List(ExecutionTag(\u0026#34;search-type\u0026#34;, \u0026#34;binary\u0026#34;)) } measure { targets.foreach(i =\u0026gt; binarySearch(dataset, i)) } // extract wall-clock times for each trial  val linearTimes: Array[Double] = linearResults.map(_.maybeTestExecution.get.responseTime * 1000).toArray val binaryTimes: Array[Double] = binaryResults.map(_.maybeTestExecution.get.responseTime * 1000).toArray // conduct stats test and obtain p-value  val statTestResult: Option[AllRegressionStatTestResults] = TwoSampleRegressionTest.testDifferenceOfMeans( binaryTimes, \u0026#34;binary search\u0026#34;, linearTimes, \u0026#34;linear search\u0026#34; ) val pValue: Double = statTestResult.get.regressionTest.pValue } Our experiment creates a randomly generated collection of 10 million integers, each in the range (0, 99). We evaluate each search algorithm by repeatedly searching for elements in another randomly generated sequence. We tag invocations in each series with a value for search-type, given by the name of the algorithm. Tags are stored in our database as additional metadata for easier discrimination of series.\nOn average, it takes 444ms to perform 10 searches using the linear algorithm. By contrast, the binary search algorithm takes only 1ms to perform the same task!\nWe can confirm this difference by inspecting our p-value. A p-value gives the probability that we could observe the same data result assuming the truth of the null hypothesis.\nIn our case, we obtain a very small p-value: 1.51E-37. The decision to reject or fail to reject the null hypothesis hinges on our desired confidence value. At the 99% confidence level, since our p-value \u0026lt; 0.01, we can successfully reject the null hypothesis and conclude that there is a significant difference between binary search and linear search!\nSummary This article has demonstrated the use of existing features in WARP to quickly validate or repudiate an experimental hypothesis. The techniques described in this article can be can be adopted to test any well-formed hypothesis.\nResearchers wishing to conduct hypothesis testing are strongly advised to base their experimental setup on idempotent tests whenever possible.\nWe have shown how to use WARP to collect measurements for experimental trials and perform some rudimentary statistical analysis to either reject or fail to reject our null hypothesis.\nThe WARP DSL makes it simple to design and execute experiments, and includes features to assist with testing for statistical significance.\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/heap_histogram/",
	"title": "Case Study: Monitoring Heap Histogram",
	"tags": [],
	"description": "",
	"content": "WARP includes a HeapHistogramCollector that can be used to monitor the effects of an experiment on the heap.\nWe use the attach API to sample the heap histogram, and write the resulting data to InfluxDB. Grafana can be used to visualize the heap measurements.\nThe HeapHistogramCollector comes in two variants: \u0026ldquo;continuous\u0026rdquo; and \u0026ldquo;bracketed\u0026rdquo;. The continuous version uses a separate thread to sample the heap at regularly specified intervals, while the bracketed version takes a heap sample before and after an experimental test.\nBoth versions support the -live flag, controlling whether or not GC should be invoked prior to sampling the heap. Using this option ensures that only live, uncollectible objects on the heap are sampled.\nThe data sampled by this collector is equivalent to that obtained by running jmap -histo \u0026lt;pid\u0026gt;, and will be similar to this example:\nnum #instances #bytes class name -------------------------------------- 1: 1414 6013016 [I 2: 793 482888 [B 3: 2502 334928 \u0026lt;constMethodKlass\u0026gt; 4: 280 274976 \u0026lt;instanceKlassKlass\u0026gt; 5: 324 227152 [D 6: 2502 200896 \u0026lt;methodKlass\u0026gt; 7: 2094 187496 [C 8: 280 172248 \u0026lt;constantPoolKlass\u0026gt; 9: 3767 139000 [Ljava.lang.Object; 10: 260 122416 \u0026lt;constantPoolCacheKlass\u0026gt; 11: 3304 112864 \u0026lt;symbolKlass\u0026gt; 12: 160 72960 java2d.Tools$3 13: 192 61440 \u0026lt;objArrayKlassKlass\u0026gt; 14: 219 55640 [F 15: 2114 50736 java.lang.String 16: 2079 49896 java.util.HashMap$Entry 17: 528 48344 [S 18: 1940 46560 java.util.Hashtable$Entry 19: 481 46176 java.lang.Class 20: 92 43424 javax.swing.plaf.metal.MetalScrollButton  Further information on jmap and the histo operation can be perused here\nHere are some results from using the continuous variant of the HeapHistogramCollector in order to view the number of Cats on the heap in Grafana. Note: byte usage on the heap is also recorded, but this Grafana query only displays the number of instances.   This data was generated by the following test:\n Instantiate a MeasurementCollectionController and a ContinuousHeapHistogramCollector Register the Collector to the Controller Begin collection Periodically populate a List[Cat] with a random number of Cats and sleep End collection  @Test def catSpec(testInfo: TestInfo): Unit = { case class Cat() val r = scala.util.Random val heapSampler: ContinuousHeapHistogramCollector = new ContinuousHeapHistogramCollector(CoreConstants.UNDEFINED_TEST_ID) // Sample the heap every 500 milliseconds instead of the default of 1 minute  heapSampler.measurementIntervalMs = 500 using testId testInfo only these collectors { heapSampler } measuring { for(i \u0026lt;- 1 to 1000) { val list: List[Cat] = List.fill(r.nextInt(1000))(new Cat) Thread.sleep(1000) } } } That graph was exposed using this Query on the Grafana Query Builder:   "
},
{
	"uri": "https://workday.github.io/warp-core/contents/extension_advanced/",
	"title": "Extending the Schema (advanced)",
	"tags": [],
	"description": "",
	"content": " Internally, we use an augmented schema with some additional proprietary columns and measurement tables.\nThis section examines our process for developing an extensible schema mechanism, and describes how the core schema can be augmented to fit custom requirements. Note that this process is quite involved, and we recommend using tags for most use cases instead of a customized schema.\nThanks to Leslie Lam for all her work on making persistence generic, and for writing the original form of this document.\nHow to Extend the Schema An augmented schema will need to have a custom version of CorePersistenceUtils. This requires several major changes to the persistence, arbiters, and collectors packages.\n Generate Slick tables to represent your new custom Warp Schema. Extend CoreQueries, and CorePersistenceUtils. Create Core and Internal implementations of AbstractQueries and AbstractPersistenceUtils Define Core and Internal implementations of each of the Arbiters Define Core and Internal implementations of each of the Collectors  In general, the Core classes/traits inherit from the Abstract traits, and augmented Internal classes/traits inherit from Core. This linear inheritance model helps decrease code duplication, as the Internal classes will only need to override methods that use custom augmented internal-only columns/tables.\nSlick We use Slick to generate classes that allow us to interact with the database using nice Scala collection-like functions. However, there are several caveats that make generic persistence difficult. (Spoiler: we solved all our problems using type classes).\nProblem 1: Case to Case Inheritance Slick generates case classes that represent a row in a table. e.g. case class BuildRow(...). If we wish to override the return type of methods defined in AbstractQueries with more specific types, we need these case classes to inherit from each other. However, case-to-case inheritance is prohibited in Scala. In order to circumvent this, we create wrapper classes for all of the core case classes. We then define implicit conversions between the case classes and the wrappers, thus allowing us to use them interchangeably and still adhere to our linear inheritance model. The wrapper classes are then extended to create their custom augmented forms.\nclass BuildRowWrapper(...) extends BuildRowLike case class BuildRow(...) extends BuildRowWrapper(...) implicit def BuildRowWrapper2BuildRow(x: BuildRowWrapper): BuildRow = ... implicit def BuildRow2BuildRowWrapper(x: BuildRow): BuildRowWrapper = ... Fortunately, all of this can be generated automatically by extending Slick\u0026rsquo;s source code generator.\nProblem 2: Slick Queries are Invariant sealed abstract class Query[+E, U, C[_]] The first type parameter +E (which is covariant) corresponds to the autogenerated Table class (which extends profile.api.Table[U]). The second type parameter U corresponds to the autogenerated case class representing the Row. The third type parameter C[_] is the container type for the query results (usually Seq). Unfortunately, because U is declared as invariant, Query[A, B', C] is not considered a subclass of Query[A, B, C], even if B' is a subclass of B. This is problematic because several of methods are defined with return types of Query. If we use these return types in the abstract definitions, then we cannot override them with more specific types in the Core/Internal implementations.\nOur workaround is using DBIO instead of Query when we need covariance. This means that we must use for-comprehensions when dealing with the results, instead of directly using collection-like methods.\nThe Database Inheritance Model While subclassing and inheritance does not really apply to databases\u0026hellip; we\u0026rsquo;re going to do it anyway. The Internal database can be considered a \u0026ldquo;subclass\u0026rdquo; of the Core database, as it completely encompasses the Core database, adding columns and tables as necessary.\nThere are essentially three \u0026ldquo;layers\u0026rdquo; to our model that occur everywhere: an abstract layer, a Core implementation layer, and an augmented Internal implementation layer, with each layer inheriting from the one above.\nOur database model is as follows:\n com.workday.warp.persistence.model.TablesLike.scala:\nA trait TablesLike containing all of the supertraits describing the required tables in the Core Schema. e.g. trait BuildRowLike { val idBuild: Int val year: Int val week: Int val buildNumber: Int val firstTested: java.sql.Timestamp val lastTested: java.sql.Timestamp }\nThis trait will also contain the type class definitions for each RowLike (See: Type Classes)\n com.workday.warp.persistence.model.Tables.scala:\nThis file contains Slick\u0026rsquo;s autogenerated representation of the Core schema. It also contains the implicit objects defining the type class for each Row.\n com.workday.warp.persistence.model.internal.Tables.scala:\nThis file contains Slick\u0026rsquo;s autogenerated representation of the Internal schema. It will look the same as the Core Tables.scala except that the case classes defined in this file will inherit from their corresponding Wrapper classes defined in the Core Tables.scala, which allows linear inheritance. e.g. case class BuildRow(override val idBuild: Int, override val year: Int, override val week: Int, override val buildNumber: Int, confidenceLevel: String, override val firstTested: java.sql.Timestamp, override val lastTested: java.sql.Timestamp) extends BuildRowWrapper(idBuild,year,week,buildNumber,firstTested,lastTested) Notice how all of the overlapping columns have an override operator, while the new column confidenceLevel does not. There is one important limitation: the columns must be defined in the same order as the original columns, otherwise the parameter list in the extends clause will be incorrect. You can insert new columns, but you may not reorder the existing ones.\n  The database model is defined as a trait, so it cannot be used directly. Instead, we have defined a separate Tables object in com.workday.warp.persistence.(internal).Tables.scala which mixes in the correct model.Tables.scala trait, defines the correct Slick JdbcProfile, and provides other convenience functions. In order to access these classes, please import this object.\nA corresponding TablesLike object has also been created in com.workday.warp.persistence.TablesLike.scala which allows importing of the TablesLike traits.\nThe reason for creating accessor objects is because if the types are mixed in, then Scala will infer path-dependent types, resulting in type mismatch errors.\nQueries, PersistenceUtils, and PersistenceAware AbstractQueries.scala This file defines all of the queries that Warp Core supports. These queries ideally should not be called directly through client code, but rather through the corresponding PersistenceUtils. In addition, you\u0026rsquo;ll notice that all of the return types are defined in terms of TablesLike.\nPersistenceAware.scala This is a trait that can be mixed in to enforce the definition of some PersistenceUtils. It defines a protected trait AbstractPersistenceUtils which defines utility functions for interacting with the database. Notice that initUtils() is called in the body. initUtils() is defined as an abstract function which should be overridden to initialize the database (create the schema, insert seed data, etc.). This ensures that the queries will not throw exceptions.\nGenerally, we define a companion object which contains the initialization logic in its body. Because companion objects are lazily initialized, we override initUtils() by calling something defined in the object.\nAbstractPersistenceUtils and its subclasses are protected traits in order to prevent clients from creating a PersistenceUtils without initializing the correct schema first. As a result, PersistenceUtils can now only be accessed by classes or traits that mix in some form of PersistenceAware.\nOriginally this was achieved by initializing the schema when PersistenceUtils was initialized, but because InternalPersistenceUtils inherits from CorePersistenceUtils, this caused initSchema() to be called multiple times, resulting in the Core schema being loaded instead of the Internal schema. With the overridden initUtils() function, we ensure that the correct initSchema() is called and only called once.\nCorePersistenceAware.scala and InternalPersistenceAware.scala These files implement the Core and Internal versions of PersistenceUtils, respectively. InternalPersistenceAware is not part of our open-source package. Note that InternalPersistenceAware extends CorePersistenceAware, allowing InternalPersistenceUtils to subclass CorePersistenceUtils. This is important because it allows us to easily create internal versions of classes (i.e. arbiters, collectors) by simply extending the core version and additionally mixing in InternalPersistenceAware.\nInitializing the Schema with Flyway Migrations The initSchema() function defined in the PersistenceUtils companion objects use flyway for database migrations.\nInternally, we use a completely different set of migrations. Any schema migrator can mix in the trait MigrateSchemaLike and call the migrate method: trait MigrateSchemaLike extends PersistenceAware { /** * Applies migrations to bring our schema up to date. * * Only supported for MySQL, not H2. * * Recursively scans `locations` to look for unapplied migration scripts. * If `locations` is empty, we\u0026#39;ll allow flyway to use its default locations. * * The type of each entry is determined by its prefix: * - no prefix (or \u0026#34;classpath:\u0026#34; prefix) indicates a location on the classpath * - a \u0026#34;filesystem:\u0026#34; prefix points to a directory on the file system * * @param locations locations we\u0026#39;ll recursively scan for migration scripts. */ def migrate(locations: Seq[String] = Seq.empty): Unit = { ... } }\nThe migration to create the Core schema lives in the default flyway directory db/migration/.\nA Failed Attempt at Generifying Persistence: Structural Types Structural Types, or \u0026ldquo;Duck Typing\u0026rdquo;, defines a type based on its interface. This allows you to pass in objects to a function as long as it satisfies the defined structure. type TestExecutionRowLikeType = { val idTestExecution: Int val idTestDefinition: Int ... }\nThis defines a TestExecutionRowLikeType to be any object that contains the corresponding values. Therefore, all of our classes and case classes satisfy this structural type. After defining all of these types, we can define our functions as follows: def writeTestExecutionQuery(row: TestExecutionRowLikeType): DBIO[TestExecutionRowLike]\nSatisfyingly clean! We then define augmented Internal versions of the structural types in order to match on the type parameter in the internal implementations. val row: InternalTestExecutionRowLikeType = maybeRow match { case r: InternalTestExecutionRowLikeType =\u0026gt; r case _ =\u0026gt; throw new Exception // Alternatively: build a InternalTestExecutionRow with default values. }\nEverything compiles and tests pass! Except for one insidious compiler warning:\na pattern match on a refinement type is unchecked case r: InternalTestExecutionRowLikeType =\u0026gt; r  Defeated by type erasure! The issue here is that structural types are erased at runtime, so the match statement actually becomes something like: val row = maybeRow match { case r: Object =\u0026gt; r case _ =\u0026gt; throw new Exception // Alternatively: build a InternalTestExecutionRow with default values. }\nwhich will always succeed. Of course, this is not ideal; the code only works if the user passes in the correct TestExecutionRow type.\nWinning the War: Type Classes Our final (and successful!) attempt was type classes. To quote the official Scala team blog:\n Type classes are a powerful and flexible concept that adds ad-hoc polymorphism to Scala.\n Our solution uses type classes to achieve the same behavior has structural typing, but it also allows us to match on the parameter type. (With the added benefit of avoiding the overhead of reflection!)\nType classes allow us to define a contract which objects must satisfy. In our case it\u0026rsquo;s the same as the structural type.\n/** Type Class for TestExecutionRowLike **/ trait TestExecutionRowLikeType[T] { def idTestExecution(row: T): Int def idTestDefinition(row: T): Int def idBuild(row: T): Int def passed(row: T): Boolean def responseTime(row: T): Double def responseTimeRequirement(row: T): Double def startTime(row: T): java.sql.Timestamp def endTime(row: T): java.sql.Timestamp } /** Prove that TestExecutionRow belongs to the type class of TestExecutionRowLikeType **/ implicit object TestExecutionRowTypeClassObject extends TestExecutionRowLikeType[TestExecutionRow] { def idTestExecution(row: TestExecutionRow): Int = row.idTestExecution def idTestDefinition(row: TestExecutionRow): Int = row.idTestDefinition def idBuild(row: TestExecutionRow): Int = row.idBuild def passed(row: TestExecutionRow): Boolean = row.passed def responseTime(row: TestExecutionRow): Double = row.responseTime def responseTimeRequirement(row: TestExecutionRow): Double = row.responseTimeRequirement def startTime(row: TestExecutionRow): java.sql.Timestamp = row.startTime def endTime(row: TestExecutionRow): java.sql.Timestamp = row.endTime def tenant(row: TestExecutionRow): String = row.tenant def teamcityBuildId(row: TestExecutionRow): Int = row.teamcityBuildId def teamcityAgentInstance(row: TestExecutionRow): Int = row.teamcityAgentInstance } /** Implicit conversion from type class to TestExecutionRow **/ implicit def TestExecutionRowFromTypeClass[T: TestExecutionRowLikeType](x: T): TestExecutionRow = TestExecutionRow(implicitly[TestExecutionRowLikeType[T]].idTestExecution(x), implicitly[TestExecutionRowLikeType[T]].idTestDefinition(x), implicitly[TestExecutionRowLikeType[T]].idBuild(x), implicitly[TestExecutionRowLikeType[T]].passed(x), implicitly[TestExecutionRowLikeType[T]].responseTime(x), implicitly[TestExecutionRowLikeType[T]].responseTimeRequirement(x), implicitly[TestExecutionRowLikeType[T]].startTime(x), implicitly[TestExecutionRowLikeType[T]].endTime(x)) override def writeTestExecutionQuery[T: TestExecutionRowLikeType](row: T): DBIO[TestExecutionRowLike] You\u0026rsquo;ll notice that the type class TestExecutionRowLikeType takes some generic T which must define functions that return the column values. We then simply need to define these functions in an implicit object. This provides implicit evidence that TestExecutionRow is TestExecutionRowLikeType. As a result, when given some object of TestExecutionRowLikeType, we can use implicitly to access the relevant columns. Of course, this is rather unwieldy, so we provide another implicit conversion that converts from the type class to the case class. Keep in mind that all this boilerplate is generated using the Slick code generator.\nIt\u0026rsquo;s important to note that the context bounds are actually syntactic sugar for implicit parameters: // This function expands to def writeTestExecutionQuery[T: TestExecutionRowLikeType](row: T): DBIO[TestExecutionRowLike] // This function def writeTestExecutionQuery[T](row: T)(implicit ev: TestExecutionRowLikeType[T]): DBIO[TestExecutionRowLike] When you call this function, Scala will look for implicit evidence that satisfies TestExecutionRowLikeType[T]. Consequently, this means that our implicit objects (e.g. TestExecutionRowTypeClassObject) need to be in scope. In general, these will be in scope as long as you import Tables.RowTypeClasses._ or TablesLike.RowTypeClasses._. We\u0026rsquo;ve included a helpful ImplicitNotFound message that will be printed at compile time if Scala cannot find the correct implicit, such as:\nCould not find an implicit value for evidence of type class TestExecutionRowLikeType[TestExecutionRow]. You might pass an (implicit ev: TestExecutionRowLikeType[TestExecutionRow]) parameter to your method or import Tables.RowTypeClasses._  The best part is that all of these type classes and implicit objects can be generated automatically with Slick\u0026rsquo;s source code generator! Hurrah! We may have lost many battles, but we have won the war! In addition, we never have to lie to the compiler with asInstanceOf, which is keeping with our Workday core value of integrity :)\nTo truly understand what\u0026rsquo;s happening with type classes, I highly recommend reading this series of blog posts by Aakash N S: * Type Classes in Scala * Better Type Class Pt. 1 * Better Type Class Pt. 2\nAn Aside about Identifiers Several of our queries look up rows based on some set of primary keys. In our augmented internal schema, this is usually the methodSignature and the confidenceLevel or the idTestDefinition and the confidenceLevel. In our open-source package, however, there is no confidenceLevel, and rows are identified by either the methodSignature or the confidenceLevel. In order to define a common abstract function that these queries can implement, we need to combine these parameters and create functions that read from an Identifier (similar to a composite key).\nWe also achieved this with type classes. trait IdentifierType[T] { def methodSignature(identifier: T): String def idTestDefinition(identifier: T): Int }\nWe then define a CoreIdentifier which implements this type class and an InternalIdentifier which includes the additional confidenceLevel field. The handling of the Identifiers is similar to how we handle the different Row type classes, where the augmented internal implementations match on the IdentifierType.\nSummary If you have a unique use case, it is possible to use WARP with a customized schema, however a simpler approach is using tags for recording additional metadata.\n Traits and type classes are autogenerated into model.TablesLike.scala Table classes, case classes, and implicit type class objects are autogenerated into model.Tables.scala Use the TablesLike and Tables objects defined in com.workday.warp.persistence to access the generated traits Mix in some form of PersistenceAware and use the provided this.persistenceUtils in order to access the persistence functions Make sure you import the correct Tables._. Also don\u0026rsquo;t forget Tables.RowClassTypes._ to bring the implicit objects in scope  This tweet by Aakash N S essentially summarizes the main lesson learned from this endeavor:\n As a rule of thumb, the answer to every question of the form “Can I do XYZ in #Scala?” is “Yeah, totally man! Just use implicits.”\n "
},
{
	"uri": "https://workday.github.io/warp-core/contents/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://workday.github.io/warp-core/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://workday.github.io/warp-core/",
	"title": "Contents",
	"tags": [],
	"description": "",
	"content": " Warp-core  Measure Your Tests: Wrap your existing JUnit tests with measurement collectors to track arbitrary custom metrics.\nJava: @WarpTest(warmups = 2, trials = 10) public void example() { yourExperiment(); }\nScala DSL: @Test def example(testInfo: TestInfo): Unit = { using testId testInfo warmups 2 trials 10 collectors { new CpuUsageCollector } measure { yourExperiment() } }\n\n  Track Historical Test Performance: All measurements are written to a SQL database for further analysis. Plot your performance over time to review historical trends.\n\n Detect Anomalies: Ships with ML-based anomaly detection. Warp-core learns your expected test performance characteristics over time, so you always know when your tests aren\u0026rsquo;t executing as expected.\n  \n Conduct Statistical Significance Testing: Unconvinced that your alternative method implementation is truly faster? Warp-core includes code for statistical analysis and comparison of two sample groups. Measure both implementations and let the p-value decide what you ship!\n"
},
{
	"uri": "https://workday.github.io/warp-core/contents/",
	"title": "Contents",
	"tags": [],
	"description": "",
	"content": "To help engineers reason about performance in a more scientific way, We’re proud to introduce WARP (Workday Automated Regression Platform): a flexible, lightweight, (mostly) functional Scala framework for collecting performance metrics and conducting sound experimental benchmarking.\nWARP features a domain specific language for describing experimental designs (conducting repeated trials for different experimental groups). Our library allows users to wrap existing tests with layers of measurement collectors that write performance metrics to a relational database. We also allow users to create arbiters to express custom failure criteria and performance requirements. One arbiter included out of the box is the RobustPcaArbiter, which uses machine learning to detect when a test is deviating from expected behavior and signal an alert.\nWe believe engineers should reason more scientifically about code performance, and we are excited to provide a platform that allows for easily describing an experiment, collecting benchmark results, and conducting statistical analyses to verify hypotheses about expected performance.\n"
},
{
	"uri": "https://workday.github.io/warp-core/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]